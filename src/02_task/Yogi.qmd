---
title: "Stellar Data Analysis"
author: 
  - Jayrup Nakawala (u2613621)
  - Yogi Patel (u2536809)
  - Jasmi Alasapuri (u2571395)
format:
  html:
    # code-fold: true
    theme: cosmo
    embed-resources: true
toc: true
jupyter: python3
execute:
  freeze: auto
---

```{python}
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, abs, radians, sin, cos, asin, degrees, floor, count, avg, stddev, rank, when, sqrt, pow
import pyspark.sql.functions as F
from pyspark.sql.window import Window
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# Initialize Spark
spark = SparkSession.builder \
    .appName("Member2_Galactic_Structure") \
    .config("spark.driver.memory", "4g") \
    .getOrCreate()

# Load the "Deep Field" Survey Data
parquet_path = "../data/gaia_survey.parquet"
df = spark.read.parquet(parquet_path)

df.describe().show()
# ==========================================
# ðŸŒŒ QUERY 2.1: The "Real" Galactic Plane
# ==========================================
# Problem: We have RA/Dec (Earth Coords). We need Galactic Latitude 'b' (Galaxy Coords).
# Solution: Trigonometric Transformation in Spark SQL.
# Formula: sin(b) = sin(dec)sin(27.13) + cos(dec)cos(27.13)cos(ra - 192.86)
# NGP Constants: RA_NGP = 192.85948, Dec_NGP = 27.12825

print("\n>>> Executing Query 2.1: Galactic Plane Motion (Coordinate Transform)...")

# Define Constants for the Transformation
ra_ngp = radians(F.lit(192.85948))
dec_ngp = radians(F.lit(27.12825))

# 1. Complex Math Projection (The "Complexity" Checkmark)
df_galactic = df.withColumn("ra_rad", radians(col("ra"))) \
    .withColumn("dec_rad", radians(col("dec"))) \
    .withColumn("sin_b", 
                (sin(col("dec_rad")) * sin(dec_ngp)) + 
                (cos(col("dec_rad")) * cos(dec_ngp) * cos(col("ra_rad") - ra_ngp))
    ).withColumn("gal_lat_b", degrees(asin(col("sin_b")))) # Convert back to degrees

# 2. Binning based on TRUE Galactic Latitude
# Plane = within +/- 15 degrees of the equator (b=0)
df_regions = df_galactic.withColumn(
    "region",
    when(abs(col("gal_lat_b")) < 15, "Galactic Plane")
    .otherwise("Galactic Halo/High-Lat")
).withColumn("total_motion", sqrt(pow(col("pmra"), 2) + pow(col("pmdec"), 2)))

# 3. Aggregation
comparison_df = df_regions.groupBy("region").agg(
    avg("total_motion").alias("avg_speed"),
    stddev("total_motion").alias("stddev_speed"),
    count("*").alias("star_count")
)

comparison_df.show()

# Visualization 2.1
pdf_comp = comparison_df.toPandas()
plt.figure(figsize=(8, 6))
sns.barplot(data=pdf_comp, x="region", y="avg_speed", palette="magma", capsize=.1)
plt.title("Query 2.1: Stellar Velocity Dispersion (Plane vs. Halo)")
plt.ylabel("Average Proper Motion (mas/yr)")
plt.show()
```



```{python}
# ==========================================
# ðŸ—ºï¸ QUERY 2.2: Star Density Sky Map
# ==========================================
print("\n>>> Executing Query 2.2: Sky Density Map...")

# 1. 2D Spatial Binning (1x1 Degree resolution for high detail)
# 2. Window Function: Rank regions to find the "Dense Core"
density_df = df.withColumn("ra_bin", floor(col("ra") / 2) * 2) \
               .withColumn("dec_bin", floor(col("dec") / 2) * 2) \
               .groupBy("ra_bin", "dec_bin") \
               .count() \
               .withColumn("rank", rank().over(Window.orderBy(col("count").desc())))

# Show top 5 densest regions (Likely the Galactic Center)
print("Top 5 Densest Sky Regions:")
density_df.filter(col("rank") <= 5).show()

# Visualization 2.2 (The Heatmap)
pdf_density = density_df.toPandas()
# Pivot for Heatmap: X=RA, Y=Dec, Z=Count
heatmap_matrix = pdf_density.pivot(index="dec_bin", columns="ra_bin", values="count").fillna(0)
heatmap_matrix.sort_index(ascending=False, inplace=True)

plt.figure(figsize=(12, 6))
sns.heatmap(heatmap_matrix, cmap="inferno", vmax=np.percentile(pdf_density['count'], 99)) # Clip outliers
plt.title("Query 2.2: The Shape of the Milky Way (Star Density)")
plt.xlabel("Right Ascension (deg)")
plt.ylabel("Declination (deg)")
plt.show()
```

```{python}
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, radians, degrees, floor, count, avg, stddev, sqrt, pow, abs, sin, cos, asin, when, lit
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
from matplotlib.colors import LogNorm

# Setup Dark Mode for that "Sci-Fi" look
plt.style.use('dark_background')
sns.set_context("notebook", font_scale=1.2)

# Initialize Spark
spark = SparkSession.builder \
    .appName("Member2_Visuals_Pro") \
    .config("spark.driver.memory", "4g") \
    .getOrCreate()

# Load Data
df = spark.read.parquet("../data/gaia_survey.parquet")

# ==========================================
# ðŸŒŒ VISUAL 2.1: Plane vs. Halo (Boxen Plot)
# ==========================================
# Why cooler? A Bar chart hides the data. A Boxen plot shows the spread, 
# tails, and outliers of the velocity distribution.

print(">>> Generating Visual 2.1 (Velocity Distribution)...")

# 1. Coordinate Transform (Same as before)
ra_ngp = radians(lit(192.85948))
dec_ngp = radians(lit(27.12825))

df_galactic = df.withColumn("ra_rad", radians(col("ra"))) \
    .withColumn("dec_rad", radians(col("dec"))) \
    .withColumn("sin_b", 
                (sin(col("dec_rad")) * sin(dec_ngp)) + 
                (cos(col("dec_rad")) * cos(dec_ngp) * cos(col("ra_rad") - ra_ngp))
    ).withColumn("gal_lat_b", degrees(asin(col("sin_b")))) \
    .withColumn("total_motion", sqrt(pow(col("pmra"), 2) + pow(col("pmdec"), 2)))

# 2. Define Regions
df_regions = df_galactic.withColumn(
    "Region",
    when(abs(col("gal_lat_b")) < 15, "Galactic Plane (Disk)")
    .otherwise("Galactic Halo (Sphere)")
)

# 3. Collect Sample (Boxen plots crash with 2M rows, so we sample 5%)
pdf_21 = df_regions.select("Region", "total_motion").sample(0.05).toPandas()

# Plotting
plt.figure(figsize=(10, 6))
# Boxenplots handle large datasets better than violins
sns.boxenplot(data=pdf_21, x="Region", y="total_motion", palette="twilight", showfliers=False)
plt.title("Query 2.1: Stellar Velocity Dispersion (Kinematics)", color="cyan", fontsize=16)
plt.ylabel("Proper Motion Speed (mas/yr)")
plt.xlabel("")
plt.grid(axis='y', linestyle='--', alpha=0.3)
plt.tight_layout()
plt.show()


# ==========================================
# ðŸ—ºï¸ VISUAL 2.2: The All-Sky Map (Mollweide)
# ==========================================
# Why cooler? It's a map of the entire sky, projected correctly. 
# We use a Log Norm because the core is 1000x denser than the halo.

# ==========================================
# ðŸ—ºï¸ VISUAL 2.2: The All-Sky Map (Mollweide) - High Res Fix
# ==========================================

print(">>> Generating Visual 2.2 (Mollweide Sky Map)...")

# 1. Binning in Spark - INCREASED RESOLUTION
# Changed factor from 10 to 60. This gives us ~1 degree resolution (closer to 60x30 grid per radian)
# This is the difference between Minecraft and 1080p.
bins_hex = df.withColumn("ra_rad", radians(col("ra") - 180)) \
             .withColumn("dec_rad", radians(col("dec"))) \
             .withColumn("ra_bin", floor(col("ra_rad") * 60) / 60) \
             .withColumn("dec_bin", floor(col("dec_rad") * 60) / 60) \
             .groupBy("ra_bin", "dec_bin") \
             .agg(count("*").alias("stars"))

pdf_22 = bins_hex.toPandas()

# Plotting
plt.figure(figsize=(15, 8)) # Made slightly larger for your 4GB GPU to render nicely
ax = plt.subplot(111, projection="mollweide")

# Create the heatmap
# Optimization: Smaller markers (s=2) and no edgecolors create a "blended" gas-cloud effect
sc = ax.scatter(
    pdf_22['ra_bin'], 
    pdf_22['dec_bin'], 
    c=pdf_22['stars'], 
    cmap='inferno', 
    s=2,              # <--- drastically reduced from 15 to prevent blockiness
    alpha=0.7,        # <--- slightly lower alpha allows layering
    norm=LogNorm(), 
    marker='h',       # <--- 'h' (hexagon) packs tighter than 's' (square) naturally
    edgecolors='none'
)

ax.grid(True, linestyle='--', alpha=0.3, color='gray')
ax.set_title("Query 2.2: The Milky Way (All-Sky Density Map)", color="orange", fontsize=16, pad=20)

# Clean up the background for the "Space" feel
ax.set_facecolor('black')
plt.colorbar(sc, label="Star Density (Log Scale)", orientation='horizontal', shrink=0.6, pad=0.05)
plt.tight_layout()
plt.show()


# ==========================================
# ðŸ“‰ VISUAL 2.3: Error Rates (Glowing Line)
# ==========================================
print(">>> Generating Visual 2.3 (Data Quality Dashboard)...")

# 1. Aggregate in Spark
df_qual = df.filter(col("phot_g_mean_mag").isNotNull()) \
    .withColumn("mag_bin", floor(col("phot_g_mean_mag"))) \
    .groupBy("mag_bin") \
    .agg(
        avg("parallax_error").alias("avg_err"),
        stddev("parallax_error").alias("std_err")
    ) \
    .orderBy("mag_bin")

pdf_23 = df_qual.toPandas()

# Plotting
fig, ax = plt.subplots(figsize=(10, 6))

# Main Line (Neon Red)
ax.plot(pdf_23['mag_bin'], pdf_23['avg_err'], color='#ff0055', linewidth=3, label='Mean Parallax Error')

# Glow Effect (Shadow lines)
for w in [5, 7, 9]:
    ax.plot(pdf_23['mag_bin'], pdf_23['avg_err'], color='#ff0055', linewidth=w, alpha=0.1)

# Error Band (Standard Deviation)
ax.fill_between(
    pdf_23['mag_bin'], 
    pdf_23['avg_err'] - pdf_23['std_err'], 
    pdf_23['avg_err'] + pdf_23['std_err'], 
    color='#ff0055', alpha=0.15
)

ax.set_title("Query 2.3: Measurement Uncertainty Profile", color="white", fontsize=16)
ax.set_xlabel("Apparent Magnitude (Dimmer ->)", fontsize=12)
ax.set_ylabel("Parallax Error (mas)", fontsize=12)
ax.grid(True, linestyle=':', alpha=0.3)
ax.legend(loc='upper left', facecolor='black', edgecolor='white')

# Annotation for the "Danger Zone"
ax.axvspan(17, 21, color='gray', alpha=0.2, hatch='///')
ax.text(18, pdf_23['avg_err'].max()*0.8, "High Noise Zone\n(Filtered out for Task 3)", 
        color="white", ha='center', fontsize=10, bbox=dict(facecolor='black', alpha=0.8))

plt.tight_layout()
plt.show()

spark.stop()
```

