{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b5a57bda",
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install pyspark"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2c4524e3",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import urllib.request\n",
                "from pyspark.sql import SparkSession\n",
                "from pyspark.sql.functions import col, expr\n",
                "\n",
                "# --- CONFIGURATION ---\n",
                "raw_dir = \"../data/raw\"\n",
                "clean_dir = \"../data/clean\"\n",
                "base_url = \"http://cdn.gea.esac.esa.int/Gaia/gdr3/gaia_source/\"\n",
                "\n",
                "files_to_download = [\n",
                "\"GaiaSource_000000-003111.csv.gz\",\n",
                "\"GaiaSource_003112-005263.csv.gz\",\n",
                "\"GaiaSource_005264-006601.csv.gz\",\n",
                "\"GaiaSource_006602-007952.csv.gz\",\n",
                "\"GaiaSource_007953-010234.csv.gz\",\n",
                "\"GaiaSource_010235-012597.csv.gz\",\n",
                "\"GaiaSource_012598-014045.csv.gz\",\n",
                "\"GaiaSource_014046-015369.csv.gz\",\n",
                "\"GaiaSource_015370-016240.csv.gz\",\n",
                "\"GaiaSource_016241-017018.csv.gz\",\n",
                "\"GaiaSource_017019-017658.csv.gz\",\n",
                "\"GaiaSource_017659-018028.csv.gz\",\n",
                "\"GaiaSource_018029-018472.csv.gz\",\n",
                "\"GaiaSource_018473-019161.csv.gz\",\n",
                "\"GaiaSource_019162-019657.csv.gz\",\n",
                "\"GaiaSource_019658-020091.csv.gz\",\n",
                "\"GaiaSource_020092-020493.csv.gz\",\n",
                "\"GaiaSource_020494-020747.csv.gz\",\n",
                "\"GaiaSource_020748-020984.csv.gz\",\n",
                "\"GaiaSource_020985-021233.csv.gz\",\n",
                "\"GaiaSource_021234-021441.csv.gz\",\n",
                "\"GaiaSource_021442-021665.csv.gz\",\n",
                "\"GaiaSource_021666-021919.csv.gz\",\n",
                "\"GaiaSource_021920-022158.csv.gz\",\n",
                "\"GaiaSource_022159-022410.csv.gz\"\n",
                "]\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "da5f56b3",
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- STEP 1: DOWNLOAD (Only runs if files missing) ---\n",
                "if not os.path.exists(raw_dir): os.makedirs(raw_dir)\n",
                "\n",
                "print(\"Checking raw files...\")\n",
                "for f in files_to_download:\n",
                "    local_path = os.path.join(raw_dir, f)\n",
                "    if not os.path.exists(local_path):\n",
                "        print(f\"Downloading {f}...\")\n",
                "        urllib.request.urlretrieve(base_url + f, local_path)\n",
                "    else:\n",
                "        print(f\"{f} already exists.\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "980d6ccd",
            "metadata": {},
            "outputs": [],
            "source": [
                "# checking the size of the raw data \n",
                "!du -sh {raw_dir}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "75cb1ee8",
            "metadata": {},
            "outputs": [],
            "source": [
                "from pyspark.sql import SparkSession\n",
                "from pyspark.sql.functions import col, expr\n",
                "\n",
                "# Initialize Spark session\n",
                "spark = SparkSession.builder \\\n",
                "    .appName(\"Stellar Data Analysis\") \\\n",
                "    .master(\"local[*]\") \\\n",
                "    .getOrCreate()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "953fea4b",
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- STEP 2: SPARK ETL ---\n",
                "print(\"Reading raw data...\")\n",
                "\n",
                "raw_df = spark.read \\\n",
                "    .option(\"header\", \"true\") \\\n",
                "    .option(\"comment\", \"#\") \\\n",
                "    .option(\"nullValue\", \"null\") \\\n",
                "    .option(\"nanValue\", \"NaN\") \\\n",
                "    .csv(raw_dir)  # Reads all chunks\n",
                "\n",
                "# defining the features we need\n",
                "cols = [\n",
                "    \"source_id\", \"ra\", \"dec\", \"parallax\", \"parallax_error\", \n",
                "    \"pmra\", \"pmdec\", \"phot_g_mean_mag\", \"bp_rp\", \"teff_gspphot\"\n",
                "]\n",
                "\n",
                "# We use a loop to apply it to all columns safely\n",
                "\n",
                "for c in cols:\n",
                "    raw_df = raw_df.withColumn(c, col(c).cast(\"double\"))\n",
                "\n",
                "clean_df = raw_df.select(cols) \\\n",
                " .filter(col(\"parallax\").isNotNull()) \\\n",
                " .filter(col(\"parallax\") > 0) \\\n",
                " .filter(col(\"ra\").isNotNull()) \\\n",
                " .filter(col(\"dec\").isNotNull())\n",
                "\n",
                "print(\"data cleaned\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b728fe77",
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- STEP 3: OPTIMIZE WRITE ---\n",
                "print(f\"Writing clean data to {clean_dir}...\")\n",
                "clean_df.coalesce(5).write.mode(\"overwrite\").parquet(clean_dir)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0d626654",
            "metadata": {},
            "outputs": [],
            "source": [
                "!du -sh {clean_dir}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "af3edbbb",
            "metadata": {},
            "outputs": [],
            "source": [
                "clean_df.describe().show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "79714170",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
